{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import quandl\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = quandl.get('BCHARTS/BITFINEXUSD') -> OLD BUGGY DATASET (OBD)\n",
    "data = quandl.get('BITFINEX/BTCUSD')\n",
    "#data = quandl.get('BITFINEX/XRPBTC')\n",
    "#data = quandl.get('BITFINEX/ETHBTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Mid</th>\n",
       "      <th>Last</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-15</th>\n",
       "      <td>513.9000</td>\n",
       "      <td>452.00</td>\n",
       "      <td>504.23500</td>\n",
       "      <td>505.0000</td>\n",
       "      <td>503.5000</td>\n",
       "      <td>504.97</td>\n",
       "      <td>21013.584774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-16</th>\n",
       "      <td>547.0000</td>\n",
       "      <td>495.00</td>\n",
       "      <td>537.50000</td>\n",
       "      <td>538.0000</td>\n",
       "      <td>537.0000</td>\n",
       "      <td>538.00</td>\n",
       "      <td>29633.358705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-17</th>\n",
       "      <td>538.5000</td>\n",
       "      <td>486.10</td>\n",
       "      <td>507.02000</td>\n",
       "      <td>508.0000</td>\n",
       "      <td>506.0400</td>\n",
       "      <td>508.00</td>\n",
       "      <td>20709.783819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-18</th>\n",
       "      <td>509.0000</td>\n",
       "      <td>474.25</td>\n",
       "      <td>483.77000</td>\n",
       "      <td>482.7500</td>\n",
       "      <td>482.7500</td>\n",
       "      <td>484.79</td>\n",
       "      <td>10458.045243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-19</th>\n",
       "      <td>513.9899</td>\n",
       "      <td>473.83</td>\n",
       "      <td>505.01065</td>\n",
       "      <td>507.4999</td>\n",
       "      <td>502.5313</td>\n",
       "      <td>507.49</td>\n",
       "      <td>8963.618369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                High     Low        Mid      Last       Bid     Ask  \\\n",
       "Date                                                                  \n",
       "2014-04-15  513.9000  452.00  504.23500  505.0000  503.5000  504.97   \n",
       "2014-04-16  547.0000  495.00  537.50000  538.0000  537.0000  538.00   \n",
       "2014-04-17  538.5000  486.10  507.02000  508.0000  506.0400  508.00   \n",
       "2014-04-18  509.0000  474.25  483.77000  482.7500  482.7500  484.79   \n",
       "2014-04-19  513.9899  473.83  505.01065  507.4999  502.5313  507.49   \n",
       "\n",
       "                  Volume  \n",
       "Date                      \n",
       "2014-04-15  21013.584774  \n",
       "2014-04-16  29633.358705  \n",
       "2014-04-17  20709.783819  \n",
       "2014-04-18  10458.045243  \n",
       "2014-04-19   8963.618369  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape before embedding: (1336, 7)\n",
      "data shape after embedding: (1321, 15, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"data shape before embedding:\",data.shape)\n",
    "\n",
    "mean = data.mean(axis=0)\n",
    "std = data.std(axis=0)\n",
    "\n",
    "# zscore normalization\n",
    "data = ( data - mean ) / std\n",
    "\n",
    "# # of days past we want skynet to see\n",
    "d = 15\n",
    "\n",
    "X = np.zeros((data.shape[0],d,data.shape[1]))\n",
    "\n",
    "# embedding d days in each DP (deslocamento)\n",
    "for i in range(d,data.shape[0]):\n",
    "    X[i,:,:] = data.iloc[i-d:i].values\n",
    "\n",
    "#removing first d lines, this ones didn't have d days past\n",
    "X = X[d:,:,:]\n",
    "\n",
    "print(\"data shape after embedding:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1336,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating labels\n",
    "Y = data['Mid'] - data.shift(1)['Mid']\n",
    "\n",
    "Y = Y > 0\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing first line: second label refers to first DP ($$ delta)\n",
    "Y = Y[1:]\n",
    "\n",
    "#removing first d days because of the embedding\n",
    "Y = Y[d:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing last registry from X, since we had to remove first DP from Y\n",
    "#specifying other dimensions for good practices - TY @lucasosouza\n",
    "X = X[: -1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1320,), (1320, 15, 7))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shapes\n",
    "Y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1320, 15, 7, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding channel layer, as expected by the convnet\n",
    "X = X.reshape((Y.shape[0],d,7,1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "#X_train = X[:-300,:,:,:]\n",
    "#X_test = X[-300:,:,:,:]\n",
    "#Y_train = Y[:-300]\n",
    "#Y_test = Y[-300:]\n",
    "\n",
    "# split dat data \n",
    "kf = KFold(n_splits=8,shuffle=True,random_state=0)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #padding same so we dont lose size\n",
    "    X = Conv2D(32,(3,3), strides=(1,1),name=\"conv0\", padding=\"same\")(X_input)\n",
    "    X = BatchNormalization(axis=3,name='bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    #X = MaxPooling2D((2,2),name='max_pool0')(X)\n",
    "    \n",
    "    #Second conv\n",
    "    X = Conv2D(64,(2,2), strides=(1,1),name=\"conv1\", padding=\"same\")(X)\n",
    "    X = BatchNormalization(axis=3,name='bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    #X = MaxPooling2D((2,2),name='max_pool1')(X)\n",
    "    \n",
    "    #Third conv\n",
    "    X = Conv2D(128,(1,1), strides=(1,1),name=\"conv2\", padding=\"same\")(X)\n",
    "    X = BatchNormalization(axis=3,name='bn2')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    #fcs\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128,activation=\"relu\")(X)\n",
    "    X = Dropout(rate=0.3, seed=0)(X)\n",
    "    X = Dense(64,activation=\"relu\")(X)\n",
    "    #X = Dropout(rate=0.3, seed=0)(X)\n",
    "    X = Dense(32,activation=\"relu\")(X)\n",
    "    #X = Dropout(rate=0.3, seed=0)(X)\n",
    "    X = Dense(1,activation=\"sigmoid\")(X)\n",
    "    \n",
    "    model = Model(inputs=X_input,outputs=X, name=\"model1\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.8928 - acc: 0.4918\n",
      "Epoch 2/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.7271 - acc: 0.5394\n",
      "Epoch 3/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.7045 - acc: 0.5333\n",
      "Epoch 4/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.7034 - acc: 0.5186\n",
      "Epoch 5/130\n",
      "1155/1155 [==============================] - 9s 7ms/step - loss: 0.7014 - acc: 0.5333\n",
      "Epoch 6/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.7013 - acc: 0.5203\n",
      "Epoch 7/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6929 - acc: 0.5420\n",
      "Epoch 8/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6920 - acc: 0.5524\n",
      "Epoch 9/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6923 - acc: 0.5186\n",
      "Epoch 10/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6929 - acc: 0.5255\n",
      "Epoch 11/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.6915 - acc: 0.5446\n",
      "Epoch 12/130\n",
      "1155/1155 [==============================] - 7s 6ms/step - loss: 0.6895 - acc: 0.5394A: 1s - loss:\n",
      "Epoch 13/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6856 - acc: 0.5532\n",
      "Epoch 14/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6878 - acc: 0.5628\n",
      "Epoch 15/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6867 - acc: 0.5576\n",
      "Epoch 16/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6849 - acc: 0.5740\n",
      "Epoch 17/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6913 - acc: 0.5550\n",
      "Epoch 18/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6870 - acc: 0.5688\n",
      "Epoch 19/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6833 - acc: 0.5662\n",
      "Epoch 20/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6786 - acc: 0.5766\n",
      "Epoch 21/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6822 - acc: 0.5567\n",
      "Epoch 22/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6809 - acc: 0.5688\n",
      "Epoch 23/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6811 - acc: 0.5636\n",
      "Epoch 24/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6687 - acc: 0.5844\n",
      "Epoch 25/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6727 - acc: 0.5775\n",
      "Epoch 26/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6638 - acc: 0.5827\n",
      "Epoch 27/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6609 - acc: 0.5896\n",
      "Epoch 28/130\n",
      "1155/1155 [==============================] - 9s 7ms/step - loss: 0.6581 - acc: 0.6035\n",
      "Epoch 29/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6474 - acc: 0.6294\n",
      "Epoch 30/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6363 - acc: 0.6398\n",
      "Epoch 31/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6321 - acc: 0.6225\n",
      "Epoch 32/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6229 - acc: 0.6416\n",
      "Epoch 33/130\n",
      "1155/1155 [==============================] - 9s 7ms/step - loss: 0.6164 - acc: 0.6494\n",
      "Epoch 34/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.6050 - acc: 0.6667\n",
      "Epoch 35/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.5806 - acc: 0.6909\n",
      "Epoch 36/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.5736 - acc: 0.6866\n",
      "Epoch 37/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.5516 - acc: 0.7022\n",
      "Epoch 38/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.5284 - acc: 0.7203\n",
      "Epoch 39/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.5187 - acc: 0.7307\n",
      "Epoch 40/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.4901 - acc: 0.7446\n",
      "Epoch 41/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.4900 - acc: 0.7385\n",
      "Epoch 42/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.4578 - acc: 0.7671\n",
      "Epoch 43/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.4326 - acc: 0.7775\n",
      "Epoch 44/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.4402 - acc: 0.7740\n",
      "Epoch 45/130\n",
      "1155/1155 [==============================] - 7s 6ms/step - loss: 0.3873 - acc: 0.7983\n",
      "Epoch 46/130\n",
      "1155/1155 [==============================] - 7s 6ms/step - loss: 0.4021 - acc: 0.7948\n",
      "Epoch 47/130\n",
      "1155/1155 [==============================] - 7s 6ms/step - loss: 0.3876 - acc: 0.7983\n",
      "Epoch 48/130\n",
      "1155/1155 [==============================] - 7s 6ms/step - loss: 0.3616 - acc: 0.8251\n",
      "Epoch 49/130\n",
      "1155/1155 [==============================] - 7s 6ms/step - loss: 0.3187 - acc: 0.8433\n",
      "Epoch 50/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2981 - acc: 0.8494\n",
      "Epoch 51/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.3137 - acc: 0.8528\n",
      "Epoch 52/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.3204 - acc: 0.8571\n",
      "Epoch 53/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2981 - acc: 0.8528\n",
      "Epoch 54/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2726 - acc: 0.8701\n",
      "Epoch 55/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2884 - acc: 0.8649\n",
      "Epoch 56/130\n",
      "1155/1155 [==============================] - 9s 7ms/step - loss: 0.2738 - acc: 0.8745\n",
      "Epoch 57/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2526 - acc: 0.8805\n",
      "Epoch 58/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2491 - acc: 0.8840\n",
      "Epoch 59/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2426 - acc: 0.8918\n",
      "Epoch 60/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2217 - acc: 0.8797\n",
      "Epoch 61/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.2112 - acc: 0.8883\n",
      "Epoch 62/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2429 - acc: 0.8779\n",
      "Epoch 63/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2383 - acc: 0.8805\n",
      "Epoch 64/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2515 - acc: 0.8857\n",
      "Epoch 65/130\n",
      "1155/1155 [==============================] - 9s 7ms/step - loss: 0.2329 - acc: 0.8883\n",
      "Epoch 66/130\n",
      "1155/1155 [==============================] - 10s 9ms/step - loss: 0.2207 - acc: 0.8918\n",
      "Epoch 67/130\n",
      "1155/1155 [==============================] - 10s 9ms/step - loss: 0.1903 - acc: 0.9022\n",
      "Epoch 68/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1938 - acc: 0.9039\n",
      "Epoch 69/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.2018 - acc: 0.9013\n",
      "Epoch 70/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.1962 - acc: 0.9117\n",
      "Epoch 71/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.1869 - acc: 0.9160\n",
      "Epoch 72/130\n",
      "1155/1155 [==============================] - 7s 6ms/step - loss: 0.1763 - acc: 0.9152\n",
      "Epoch 73/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.1804 - acc: 0.9117\n",
      "Epoch 74/130\n",
      "1155/1155 [==============================] - 10s 9ms/step - loss: 0.1731 - acc: 0.9048\n",
      "Epoch 75/130\n",
      "1155/1155 [==============================] - 12s 10ms/step - loss: 0.1766 - acc: 0.9255\n",
      "Epoch 76/130\n",
      "1155/1155 [==============================] - 10s 8ms/step - loss: 0.1705 - acc: 0.9169\n",
      "Epoch 77/130\n",
      "1155/1155 [==============================] - 8s 6ms/step - loss: 0.1721 - acc: 0.9108\n",
      "Epoch 78/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.1912 - acc: 0.9048\n",
      "Epoch 79/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.1444 - acc: 0.9281\n",
      "Epoch 80/130\n",
      "1155/1155 [==============================] - 9s 7ms/step - loss: 0.1620 - acc: 0.9229\n",
      "Epoch 81/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.1727 - acc: 0.9281\n",
      "Epoch 82/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.1815 - acc: 0.9203\n",
      "Epoch 83/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.1507 - acc: 0.9229\n",
      "Epoch 84/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.1419 - acc: 0.9290\n",
      "Epoch 85/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.1395 - acc: 0.9394\n",
      "Epoch 86/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.1580 - acc: 0.9273\n",
      "Epoch 87/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.1782 - acc: 0.9264\n",
      "Epoch 88/130\n",
      "1155/1155 [==============================] - 8s 7ms/step - loss: 0.1518 - acc: 0.9273\n",
      "Epoch 89/130\n",
      "1155/1155 [==============================] - 9s 8ms/step - loss: 0.1566 - acc: 0.9290\n",
      "Epoch 90/130\n",
      "1155/1155 [==============================] - 10s 9ms/step - loss: 0.1889 - acc: 0.9238\n",
      "Epoch 91/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1547 - acc: 0.9316\n",
      "Epoch 92/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1380 - acc: 0.9377\n",
      "Epoch 93/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1038 - acc: 0.9532\n",
      "Epoch 94/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1361 - acc: 0.9255\n",
      "Epoch 95/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1705 - acc: 0.9290\n",
      "Epoch 96/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1526 - acc: 0.9342\n",
      "Epoch 97/130\n",
      "1155/1155 [==============================] - ETA: 0s - loss: 0.1270 - acc: 0.942 - 11s 10ms/step - loss: 0.1269 - acc: 0.9429\n",
      "Epoch 98/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1320 - acc: 0.9420\n",
      "Epoch 99/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1622 - acc: 0.9221\n",
      "Epoch 100/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1300 - acc: 0.9403\n",
      "Epoch 101/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1514 - acc: 0.9385\n",
      "Epoch 102/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1328 - acc: 0.9368\n",
      "Epoch 103/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1338 - acc: 0.9437\n",
      "Epoch 104/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1346 - acc: 0.9377\n",
      "Epoch 105/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1355 - acc: 0.9455\n",
      "Epoch 106/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1489 - acc: 0.9437\n",
      "Epoch 107/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1163 - acc: 0.9524\n",
      "Epoch 108/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1196 - acc: 0.9463\n",
      "Epoch 109/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1124 - acc: 0.9541\n",
      "Epoch 110/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1376 - acc: 0.9463\n",
      "Epoch 111/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1228 - acc: 0.9524\n",
      "Epoch 112/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1156 - acc: 0.9541\n",
      "Epoch 113/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1306 - acc: 0.9489\n",
      "Epoch 114/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1376 - acc: 0.9437\n",
      "Epoch 115/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1401 - acc: 0.9385\n",
      "Epoch 116/130\n",
      "1155/1155 [==============================] - 11s 9ms/step - loss: 0.1161 - acc: 0.9506\n",
      "Epoch 117/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1387 - acc: 0.9489\n",
      "Epoch 118/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1404 - acc: 0.9420\n",
      "Epoch 119/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1371 - acc: 0.9411\n",
      "Epoch 120/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1179 - acc: 0.9515\n",
      "Epoch 121/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1167 - acc: 0.9463\n",
      "Epoch 122/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1070 - acc: 0.9593\n",
      "Epoch 123/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.0989 - acc: 0.9593\n",
      "Epoch 124/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1006 - acc: 0.9593\n",
      "Epoch 125/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.0943 - acc: 0.9619\n",
      "Epoch 126/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1194 - acc: 0.9506\n",
      "Epoch 127/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.1171 - acc: 0.9489\n",
      "Epoch 128/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.0857 - acc: 0.9567\n",
      "Epoch 129/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.0990 - acc: 0.9550\n",
      "Epoch 130/130\n",
      "1155/1155 [==============================] - 11s 10ms/step - loss: 0.0981 - acc: 0.9610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f1c0c15f8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model, time for witchcraft\n",
    "model = model(X[0].shape)\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x = X_train, y = Y_train, epochs = 130, batch_size = 8,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 0s 1ms/step\n",
      "\n",
      "Loss = 2.29252923041\n",
      "Acc = 0.539393939575\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(x = X_test, y = Y_test)\n",
    "print()\n",
    "print(\"Loss = \" + str(preds[0]))\n",
    "print(\"Acc = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1f19a96e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEruISI1RYhcRqTH/H/iTg6vwRspFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f1d552320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['Mid'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
